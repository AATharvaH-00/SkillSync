{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "706445d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model trained and saved to models/job_recommender.joblib\n",
      "\n",
      "ðŸ“‹ FINAL RECOMMENDATIONS:\n",
      "ðŸ”¹ Data Scientist at Google (70.7% Match)\n",
      "   ðŸ’¡ Tip: Learn machine learning to improve fit.\n",
      "ðŸ”¹ Cloud Engineer at Amazon (0.0% Match)\n",
      "   ðŸ’¡ Tip: Learn aws, linux, docker to improve fit.\n",
      "ðŸ”¹ Web Developer at Meta (0.0% Match)\n",
      "   ðŸ’¡ Tip: Learn react, javascript, css to improve fit.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import joblib\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class JobRecommendationModel:\n",
    "    \"\"\"\n",
    "    Final Model following Phase 2 Workflow:\n",
    "    Data Cleaning -> Data Preprocessing -> Job Recommendation -> Skill Gap Analysis\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path=\"models/job_recommender.joblib\"):\n",
    "        self.model_path = model_path\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.skill_mlb = MultiLabelBinarizer()\n",
    "        self.job_embeddings = None\n",
    "        self.job_metadata = None\n",
    "\n",
    "    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Step 1: Data Cleaning (Removes nulls and standardizes text)\"\"\"\n",
    "        df = df.dropna(subset=['title', 'skills']).copy()\n",
    "        # Convert skills to lowercase list for consistent matching\n",
    "        df['skills_list'] = df['skills'].apply(\n",
    "            lambda x: [s.strip().lower() for s in str(x).split(',')]\n",
    "        )\n",
    "        df['description'] = df['description'].fillna('')\n",
    "        df['location'] = df['location'].fillna('Remote')\n",
    "        return df\n",
    "\n",
    "    def _create_features(self, df: pd.DataFrame):\n",
    "        \"\"\"Step 2: Data Preprocessing (TF-IDF + Skill Encoding)\"\"\"\n",
    "        # Text Context (Title + Description)\n",
    "        text_corpus = df['title'] + \" \" + df['description']\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "        text_features = self.tfidf_vectorizer.fit_transform(text_corpus).toarray()\n",
    "\n",
    "        # Skill Encoding\n",
    "        skill_features = self.skill_mlb.fit_transform(df['skills_list'])\n",
    "\n",
    "        # Combine and Normalize\n",
    "        combined = np.hstack([text_features, skill_features])\n",
    "        # L2 Normalization makes Cosine Similarity just a dot product\n",
    "        norm = np.linalg.norm(combined, axis=1, keepdims=True) + 1e-8\n",
    "        return combined / norm\n",
    "\n",
    "    def train(self, csv_path: str):\n",
    "        \"\"\"Full Training Pipeline\"\"\"\n",
    "        if not os.path.exists(csv_path):\n",
    "            raise FileNotFoundError(f\"Missing dataset at {csv_path}\")\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df = self._clean_data(df)\n",
    "        self.job_embeddings = self._create_features(df)\n",
    "        self.job_metadata = df.reset_index(drop=True)\n",
    "\n",
    "        # Ensure directory exists and save\n",
    "        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)\n",
    "        joblib.dump({\n",
    "            'tfidf': self.tfidf_vectorizer,\n",
    "            'mlb': self.skill_mlb,\n",
    "            'embeddings': self.job_embeddings,\n",
    "            'metadata': self.job_metadata\n",
    "        }, self.model_path)\n",
    "        print(f\"âœ… Model trained and saved to {self.model_path}\")\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Load the saved components\"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            return False\n",
    "        data = joblib.load(self.model_path)\n",
    "        self.tfidf_vectorizer = data['tfidf']\n",
    "        self.skill_mlb = data['mlb']\n",
    "        self.job_embeddings = data['embeddings']\n",
    "        self.job_metadata = data['metadata']\n",
    "        return True\n",
    "\n",
    "    def recommend(self, user_skills: List[str], top_k=3) -> List[Dict]:\n",
    "        \"\"\"Step 3: Job Recommendation (Similarity Matching)\"\"\"\n",
    "        if self.job_embeddings is None and not self.load():\n",
    "            return []\n",
    "\n",
    "        # Preprocess User Input\n",
    "        user_skills_clean = [s.strip().lower() for s in user_skills]\n",
    "        user_text_vec = self.tfidf_vectorizer.transform([\" \".join(user_skills_clean)]).toarray()\n",
    "        \n",
    "        # Handle unseen skills gracefully\n",
    "        try:\n",
    "            user_skill_vec = self.skill_mlb.transform([user_skills_clean])\n",
    "        except ValueError: # If skills are completely new\n",
    "            user_skill_vec = np.zeros((1, len(self.skill_mlb.classes_)))\n",
    "\n",
    "        user_vec = np.hstack([user_text_vec, user_skill_vec])\n",
    "        user_vec = user_vec / (np.linalg.norm(user_vec) + 1e-8)\n",
    "\n",
    "        # Calculate Similarity\n",
    "        scores = np.dot(self.job_embeddings, user_vec.T).flatten()\n",
    "        top_idx = scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "        # Step 4: Skill Gap Analysis\n",
    "        recommendations = []\n",
    "        for idx in top_idx:\n",
    "            job = self.job_metadata.iloc[idx]\n",
    "            job_skills = set(job['skills_list'])\n",
    "            user_set = set(user_skills_clean)\n",
    "            \n",
    "            missing = list(job_skills - user_set)\n",
    "            match_pct = (len(job_skills - set(missing)) / len(job_skills)) * 100\n",
    "\n",
    "            recommendations.append({\n",
    "                \"job_title\": job['title'],\n",
    "                \"company\": job['company'],\n",
    "                \"match_score\": round(float(scores[idx]) * 100, 1),\n",
    "                \"skill_match_percent\": round(match_pct, 1),\n",
    "                \"missing_skills\": missing[:3], # Show top 3 missing\n",
    "                \"location\": job['location']\n",
    "            })\n",
    "        return recommendations\n",
    "\n",
    "# --- Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Create directory structure\n",
    "    os.makedirs(\"datasets\", exist_ok=True)\n",
    "    \n",
    "    # 2. Sample Data for testing\n",
    "    data = {\n",
    "        \"title\": [\"Data Scientist\", \"Web Developer\", \"Cloud Engineer\"],\n",
    "        \"company\": [\"Google\", \"Meta\", \"Amazon\"],\n",
    "        \"description\": [\"Analyze data and build ML models\", \"Build React apps\", \"Manage AWS infra\"],\n",
    "        \"skills\": [\"Python, SQL, Machine Learning\", \"JavaScript, React, CSS\", \"AWS, Docker, Linux\"],\n",
    "        \"location\": [\"Remote\", \"NY\", \"Seattle\"]\n",
    "    }\n",
    "    pd.DataFrame(data).to_csv(\"datasets/job_postings.csv\", index=False)\n",
    "\n",
    "    # 3. Initialize and Run\n",
    "    model = JobRecommendationModel()\n",
    "    model.train(\"datasets/job_postings.csv\")\n",
    "    \n",
    "    # 4. Get Recommendations\n",
    "    results = model.recommend([\"Python\", \"SQL\"])\n",
    "    \n",
    "    print(\"\\nðŸ“‹ FINAL RECOMMENDATIONS:\")\n",
    "    for r in results:\n",
    "        print(f\"ðŸ”¹ {r['job_title']} at {r['company']} ({r['match_score']}% Match)\")\n",
    "        if r['missing_skills']:\n",
    "            print(f\"   ðŸ’¡ Tip: Learn {', '.join(r['missing_skills'])} to improve fit.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
